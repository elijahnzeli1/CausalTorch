{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CausalTorch: Video Generation with Temporal Causality\n",
    "\n",
    "This notebook demonstrates how to use CausalTorch to generate video sequences with causal temporal constraints. We'll implement a battle scene example where temporal causal rules like \"arrow hit â†’ soldier fall\" are enforced across frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CausalTorch if not already installed\n",
    "%pip install -e ..\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import imageio\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Import CausalTorch components\n",
    "from causaltorch.models import CNSG_VideoGenerator\n",
    "from causaltorch.rules import CausalRuleSet\n",
    "from causaltorch.metrics import temporal_consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Temporal Causal Rules\n",
    "\n",
    "Temporal causal rules define how certain events trigger effects over time. For example, an arrow hit at frame t should cause a soldier to fall at frame t+3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporal causal rules for battle scene\n",
    "battle_rules = {\n",
    "    \"hoof_contact\": {\n",
    "        \"effect\": \"dust\",\n",
    "        \"strength\": 0.8,\n",
    "        \"temporal_offset\": 0,  # Immediate effect\n",
    "        \"duration\": 3  # Dust lasts 3 frames\n",
    "    },\n",
    "    \"arrow_hit\": {\n",
    "        \"effect\": \"soldier_fall\",\n",
    "        \"strength\": 0.9,\n",
    "        \"temporal_offset\": 3,  # Effect happens 3 frames later\n",
    "        \"duration\": 10  # Fall animation lasts 10 frames\n",
    "    },\n",
    "    \"explosion\": {\n",
    "        \"effect\": \"smoke_cloud\",\n",
    "        \"strength\": 0.95,\n",
    "        \"temporal_offset\": 1,\n",
    "        \"duration\": 15  # Smoke lasts 15 frames\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create rule set for visualization\n",
    "rule_set = CausalRuleSet(battle_rules)\n",
    "\n",
    "# Visualize the causal graph\n",
    "rule_set.visualize(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Video Generator Model\n",
    "\n",
    "We'll create a CNSG_VideoGenerator model that enforces temporal causal constraints during video generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the video generator model\n",
    "frame_size = (64, 64)  # Height, width\n",
    "latent_dim = 16  # Latent space dimension\n",
    "model = CNSG_VideoGenerator(frame_size=frame_size, latent_dim=latent_dim, causal_rules=battle_rules)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Metadata for Causal Events\n",
    "\n",
    "We need to specify when causal events occur in our video sequence. For example, when hooves hit the ground or when arrows hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_battle_metadata(num_frames=48):\n",
    "    \"\"\"Create metadata for a battle scene with causal events\"\"\"\n",
    "    metadata = {}\n",
    "    \n",
    "    # Horse hoof contacts ground every 6 frames (gallop rhythm)\n",
    "    metadata[\"hoof_contact\"] = [1.0 if i % 6 == 0 else 0.0 for i in range(num_frames)]\n",
    "    \n",
    "    # Arrow hits at specific frames\n",
    "    arrow_hits = [12, 28]  # Frames where arrows hit\n",
    "    metadata[\"arrow_hit\"] = [1.0 if i in arrow_hits else 0.0 for i in range(num_frames)]\n",
    "    \n",
    "    # Explosion at frame 20\n",
    "    explosion_frame = 20\n",
    "    metadata[\"explosion\"] = [1.0 if i == explosion_frame else 0.0 for i in range(num_frames)]\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Create metadata for a 48-frame sequence\n",
    "battle_metadata = create_battle_metadata(num_frames=48)\n",
    "\n",
    "# Visualize the metadata events\n",
    "plt.figure(figsize=(12, 4))\n",
    "events = list(battle_metadata.keys())\n",
    "for i, event in enumerate(events):\n",
    "    plt.subplot(len(events), 1, i+1)\n",
    "    plt.plot(battle_metadata[event], 'o-')\n",
    "    plt.ylabel(event)\n",
    "    plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('Frame')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate a Battle Scene Video\n",
    "\n",
    "Now we'll generate a video sequence with our causal constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_battle_video(model, metadata, num_frames=48):\n",
    "    \"\"\"Generate a battle video with causal constraints\"\"\"\n",
    "    # Create initial inputs\n",
    "    batch_size = 1\n",
    "    # Start with a simple scene (could be a more realistic frame in a real implementation)\n",
    "    initial_frame = torch.rand(batch_size, 3, model.frame_size[0], model.frame_size[1]) * 0.5 + 0.25\n",
    "    initial_latent = torch.zeros(batch_size, model.latent_dim)\n",
    "    \n",
    "    # Add some randomness to the latent vector\n",
    "    initial_latent[:, 0:5] = torch.randn(batch_size, 5) * 0.5\n",
    "    \n",
    "    # Generate video\n",
    "    with torch.no_grad():\n",
    "        video = model(initial_frame, initial_latent, seq_length=num_frames, metadata=metadata)\n",
    "    \n",
    "    return video\n",
    "\n",
    "# Generate battle video\n",
    "num_frames = 48\n",
    "battle_video = generate_battle_video(model, battle_metadata, num_frames=num_frames)\n",
    "\n",
    "# Get video shape\n",
    "print(f\"Generated video shape: {battle_video.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize the Generated Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video_frames(video, num_frames=8):\n",
    "    \"\"\"Display selected frames from the video\"\"\"\n",
    "    # Convert to numpy for display\n",
    "    video_np = video.squeeze().permute(0, 2, 3, 1).cpu().numpy()\n",
    "    \n",
    "    # Select evenly spaced frames\n",
    "    indices = np.linspace(0, len(video_np)-1, num_frames, dtype=int)\n",
    "    \n",
    "    # Display frames\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(2, num_frames//2, i+1)\n",
    "        plt.imshow(video_np[idx])\n",
    "        plt.title(f\"Frame {idx}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display selected frames\n",
    "display_video_frames(battle_video, num_frames=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_animation(video):\n",
    "    \"\"\"Create an animation from the video frames\"\"\"\n",
    "    video_np = video.squeeze().permute(0, 2, 3, 1).cpu().numpy()\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Create initial image\n",
    "    img = ax.imshow(video_np[0])\n",
    "    \n",
    "    # Animation function\n",
    "    def animate(i):\n",
    "        img.set_array(video_np[i])\n",
    "        return [img]\n",
    "    \n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(video_np), interval=100, blit=True)\n",
    "    plt.close()\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "# Create and display animation\n",
    "create_animation(battle_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Generated Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(video, filename=\"battle_scene.mp4\", fps=12):\n",
    "    \"\"\"Save the generated video to a file\"\"\"\n",
    "    video_np = video.squeeze().permute(0, 2, 3, 1).cpu().numpy()\n",
    "    \n",
    "    # Convert to uint8 (0-255)\n",
    "    video_np = (video_np * 255).astype(np.uint8)\n",
    "    \n",
    "    # Save as MP4\n",
    "    imageio.mimsave(filename, video_np, fps=fps)\n",
    "    print(f\"Video saved to {filename}\")\n",
    "\n",
    "# Save the video\n",
    "save_video(battle_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Measure Temporal Consistency and Causal Fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_causal_events(video, metadata):\n",
    "    \"\"\"Measure the visible effects of causal events in the video\"\"\"\n",
    "    video_np = video.squeeze().permute(0, 2, 3, 1).cpu().numpy()\n",
    "    num_frames = video_np.shape[0]\n",
    "    \n",
    "    # Get key frames for each causal event\n",
    "    hoof_frames = [i for i in range(num_frames) if metadata[\"hoof_contact\"][i] > 0.5]\n",
    "    arrow_frames = [i for i in range(num_frames) if metadata[\"arrow_hit\"][i] > 0.5]\n",
    "    explosion_frame = [i for i in range(num_frames) if metadata[\"explosion\"][i] > 0.5][0]\n",
    "    \n",
    "    # Analyze dust after hoof contact (look at lower part of frames)\n",
    "    dust_intensity = []\n",
    "    for frame in hoof_frames:\n",
    "        if frame + 1 < num_frames:\n",
    "            # Calculate motion in ground area (bottom 20% of frame)\n",
    "            ground_area = video_np[frame + 1, int(0.8 * video_np.shape[1]):, :, :]\n",
    "            dust = np.mean(ground_area)\n",
    "            dust_intensity.append(dust)\n",
    "    \n",
    "    # Calculate temporal consistency\n",
    "    tc_score = temporal_consistency(video)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Temporal Consistency Score: {tc_score:.4f}\")\n",
    "    print(f\"Average dust intensity after hoof contact: {np.mean(dust_intensity):.4f}\")\n",
    "    print(f\"Causal events: Hoof contacts at frames {hoof_frames}\")\n",
    "    print(f\"              Arrow hits at frames {arrow_frames}\")\n",
    "    print(f\"              Explosion at frame {explosion_frame}\")\n",
    "\n",
    "# Measure causal effects\n",
    "measure_causal_events(battle_video, battle_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Counterfactual Intervention: \"What if there was no explosion?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create alternative metadata with no explosion\n",
    "alternative_metadata = create_battle_metadata(num_frames=48)\n",
    "alternative_metadata[\"explosion\"] = [0.0 for _ in range(48)]  # Remove explosion\n",
    "\n",
    "# Generate counterfactual video\n",
    "counterfactual_video = generate_battle_video(model, alternative_metadata, num_frames=48)\n",
    "\n",
    "# Display frames around where the explosion would have been\n",
    "explosion_frame = 20\n",
    "window = 2  # Show frames before and after\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "# Original video (with explosion)\n",
    "video_np = battle_video.squeeze().permute(0, 2, 3, 1).cpu().numpy()\n",
    "for i in range(explosion_frame - window, explosion_frame + window + 1):\n",
    "    plt.subplot(2, 5, i - explosion_frame + window + 1)\n",
    "    plt.imshow(video_np[i])\n",
    "    plt.title(f\"Original: Frame {i}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# Counterfactual video (no explosion)\n",
    "cf_video_np = counterfactual_video.squeeze().permute(0, 2, 3, 1).cpu().numpy()\n",
    "for i in range(explosion_frame - window, explosion_frame + window + 1):\n",
    "    plt.subplot(2, 5, i - explosion_frame + window + 6)\n",
    "    plt.imshow(cf_video_np[i])\n",
    "    plt.title(f\"Counterfactual: Frame {i}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"Counterfactual: What if there was no explosion?\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"battle_video_generator.pt\")\n",
    "print(\"Model saved to battle_video_generator.pt\")\n",
    "\n",
    "# Load the model (for demonstration)\n",
    "loaded_model = CNSG_VideoGenerator(frame_size=frame_size, latent_dim=latent_dim, causal_rules=battle_rules)\n",
    "loaded_model.load_state_dict(torch.load(\"battle_video_generator.pt\"))\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how CausalTorch can be used to generate video sequences with temporal causal constraints. Key takeaways:\n",
    "\n",
    "1. We defined temporal causal rules with appropriate offsets and durations.\n",
    "2. We created a video generator model that enforces these rules during generation.\n",
    "3. We specified when causal events occur using metadata.\n",
    "4. We generated a battle scene video with causally consistent effects.\n",
    "5. We measured the temporal consistency and verified causal effects.\n",
    "6. We performed a counterfactual intervention to see how the video changes when an event is removed.\n",
    "\n",
    "This approach enables more realistic and logically consistent video generation, especially for scenarios where temporal causality is important, like simulations, game development, and film production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
