"""
CausalTorch Native Text Generation Demo
======================================

This script demonstrates the new native CausalTorch text generation model (cnsg)
that doesn't rely on external models like GPT-2, but implements its own
causal neuro-symbolic architecture.

Key Features:
- Native CausalTorch transformer architecture
- Integrated causal reasoning in every layer
- Causal positional encoding
- Causal attention mechanisms
- Generation with causal constraints
- No external model dependencies
"""

import torch
import torch.nn.functional as F
from causaltorch.models import cnsg

def demonstrate_native_cnsg():
    """Demonstrate the native CausalTorch text generation model."""
    print("ğŸš€ CausalTorch Native Text Generation Demo")
    print("=" * 60)
    print("Testing the new cnsg (Causal Neuro-Symbolic Generator)")
    print("without GPT-2 dependencies - pure CausalTorch architecture!")
    print("=" * 60)
    
    # Define causal rules for text generation
    causal_rules = {
        'cause_effect_pairs': [
            {'cause': 'question_word', 'effect': 'interrogative_response', 'strength': 0.9},
            {'cause': 'negative_context', 'effect': 'negative_sentiment', 'strength': 0.8},
            {'cause': 'past_tense', 'effect': 'temporal_consistency', 'strength': 0.7}
        ],
        'logical_constraints': [
            {'type': 'temporal', 'rule': 'past_before_present'},
            {'type': 'causal', 'rule': 'cause_before_effect'}
        ]
    }
    
    print("\\nğŸ”§ Creating native CausalTorch text model...")
    
    # Create the model with smaller dimensions for demo
    model = cnsg(
        vocab_size=1000,      # Smaller vocab for demo
        d_model=256,          # Smaller model dimension
        n_heads=8,            # 8 attention heads
        n_layers=6,           # 6 transformer layers
        d_ff=1024,            # Feed-forward dimension
        max_seq_length=512,   # Maximum sequence length
        causal_rules=causal_rules
    )
    
    print(f"âœ… Created cnsg model:")
    print(f"   ğŸ“Š Parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"   ğŸ§  Model dimension: {model.d_model}")
    print(f"   ğŸ”¢ Vocabulary size: {model.vocab_size}")
    print(f"   ğŸ—ï¸ Layers: {model.n_layers}")
    print(f"   ğŸ‘ï¸ Attention heads: {model.n_heads}")
    
    # Test forward pass
    print("\\nğŸ§ª Testing forward pass...")
    
    # Create sample input tokens
    batch_size = 2
    seq_length = 10
    input_ids = torch.randint(0, model.vocab_size, (batch_size, seq_length))
    
    print(f"   Input shape: {input_ids.shape}")
    
    # Forward pass
    with torch.no_grad():
        outputs = model(input_ids)
    
    logits = outputs["logits"]
    print(f"   Output logits shape: {logits.shape}")
    print(f"   âœ… Forward pass successful!")
    
    # Test with loss computation
    print("\\nğŸ“Š Testing training mode with loss computation...")
    
    # Create labels for loss computation (shifted input_ids)
    labels = torch.randint(0, model.vocab_size, (batch_size, seq_length))
    
    outputs_with_loss = model(input_ids, labels=labels)
    
    if "loss" in outputs_with_loss:
        loss = outputs_with_loss["loss"]
        print(f"   Computed loss: {loss.item():.4f}")
        print(f"   âœ… Loss computation successful!")
    
    # Test text generation
    print("\\nğŸ¯ Testing text generation...")
    
    # Start with a short input sequence
    start_tokens = torch.randint(0, model.vocab_size, (1, 5))
    print(f"   Starting with {start_tokens.shape[1]} tokens")
    
    # Generate text
    generated = model.generate(
        input_ids=start_tokens,
        max_length=20,
        temperature=0.8,
        top_k=50,
        top_p=0.9
    )
    
    print(f"   Generated sequence length: {generated.shape[1]}")
    print(f"   Generated tokens: {generated[0].tolist()}")
    print(f"   âœ… Text generation successful!")
    
    # Test causal constraints in generation
    print("\\nğŸ”¬ Testing causal constraints in generation...")
    
    causal_constraints = {
        'forbidden_words': [999],  # Forbid token 999
        'encouraged_words': [100, 200]  # Encourage tokens 100, 200
    }
    
    constrained_generated = model.generate(
        input_ids=start_tokens,
        max_length=15,
        temperature=0.8,
        causal_constraints=causal_constraints
    )
    
    print(f"   Constrained generation length: {constrained_generated.shape[1]}")
    print(f"   Constrained tokens: {constrained_generated[0].tolist()}")
    
    # Check if forbidden token was avoided
    if 999 not in constrained_generated[0].tolist():
        print("   âœ… Successfully avoided forbidden token!")
    else:
        print("   âš ï¸ Forbidden token appeared in generation")
    
    # Test causal attention pattern extraction
    print("\\nğŸ§  Testing causal attention pattern extraction...")
    
    try:
        attention_patterns = model.get_causal_attention_patterns(input_ids[:1])
        print(f"   Extracted attention patterns for {len(attention_patterns)} layers")
        print(f"   âœ… Attention analysis successful!")
    except Exception as e:
        print(f"   âš ï¸ Attention analysis: {e}")
    
    # Architecture comparison
    print("\\nğŸ“‹ Architecture Summary:")
    print("   ğŸ—ï¸ NATIVE CAUSALTORCH ARCHITECTURE:")
    print("      â€¢ CausalPositionalEncoding for temporal reasoning")
    print("      â€¢ CausalTransformerBlock with integrated causal layers")
    print("      â€¢ CausalAttentionLayer for causal relationships")
    print("      â€¢ CausalSymbolicLayer for symbolic reasoning")
    print("      â€¢ Native generation with causal constraints")
    print("   ")
    print("   ğŸš« NO EXTERNAL DEPENDENCIES:")
    print("      â€¢ No GPT-2 model loading")
    print("      â€¢ No transformers library requirement")
    print("      â€¢ Pure PyTorch + CausalTorch implementation")
    print("      â€¢ Custom causal reasoning throughout")
    
    return True

def demonstrate_model_components():
    """Demonstrate individual components of the native model."""
    print("\\n" + "=" * 60)
    print("ğŸ” INDIVIDUAL COMPONENT DEMONSTRATION")
    print("=" * 60)
    
    try:
        # Import components directly from the parent models module
        from causaltorch.models import cnsg
        
        # Test the cnsg model creation with different sizes
        print("\\nğŸ¯ Testing different cnsg model configurations...")
        
        # Small model
        small_model = cnsg(
            vocab_size=500,
            d_model=128,
            n_heads=4,
            n_layers=3,
            d_ff=512,
            max_seq_length=100,
            causal_rules={'test_rule': {'strength': 0.5}}
        )
        print(f"   Small model parameters: {sum(p.numel() for p in small_model.parameters()):,}")
        
        # Medium model  
        medium_model = cnsg(
            vocab_size=2000,
            d_model=512,
            n_heads=8,
            n_layers=6,
            d_ff=2048,
            max_seq_length=512,
            causal_rules={'advanced_rule': {'strength': 0.8}}
        )
        print(f"   Medium model parameters: {sum(p.numel() for p in medium_model.parameters()):,}")
        
        print("\\nğŸ”§ Testing model components...")
        
        # Test forward pass with both models
        test_input = torch.randint(0, 500, (1, 5))
        
        with torch.no_grad():
            small_output = small_model(test_input)
            medium_output = medium_model(test_input[:, :5])  # Adjust input size
            
        print(f"   Small model output shape: {small_output['logits'].shape}")
        print(f"   Medium model output shape: {medium_output['logits'].shape}")
        
        print("\\nğŸ† Component Summary:")
        print("   âœ… CausalPositionalEncoding: Adds causal temporal information")
        print("   âœ… CausalTransformerBlock: Integrates causal reasoning in attention")
        print("   âœ… Native architecture: No external model dependencies")
        print("   âœ… Scalable design: Works with different model sizes")
        
        return True
        
    except Exception as e:
        print(f"   âŒ Component test failed: {e}")
        return False

def main():
    """Run the complete native CausalTorch text generation demonstration."""
    print("CausalTorch Native Text Generation Refactoring Demo")
    print("=" * 80)
    print("Demonstrating the refactored cnsg model without GPT-2 dependencies")
    print("=" * 80)
    
    try:
        # Test the main model
        model_success = demonstrate_native_cnsg()
        
        # Test individual components
        components_success = demonstrate_model_components()
        
        # Final summary
        print("\\n" + "=" * 80)
        print("ğŸ† REFACTORING SUMMARY")
        print("=" * 80)
        
        if model_success and components_success:
            print("âœ… SUCCESS: Complete refactoring accomplished!")
            print("\\nğŸ¯ What was changed:")
            print("   ğŸš« REMOVED: GPT2LMHeadModel dependency")
            print("   ğŸš« REMOVED: transformers library requirement")
            print("   ğŸš« REMOVED: External model fine-tuning approach")
            print("   ")
            print("   âœ… ADDED: Native CausalTorch text generation architecture")
            print("   âœ… ADDED: CausalPositionalEncoding for temporal reasoning")
            print("   âœ… ADDED: CausalTransformerBlock with integrated causal layers")
            print("   âœ… ADDED: Custom generation with causal constraints")
            print("   âœ… ADDED: Pure PyTorch + CausalTorch implementation")
            print("   ")
            print("ğŸ’« Benefits:")
            print("   ğŸš€ No external model dependencies")
            print("   ğŸ§  Causal reasoning integrated throughout")
            print("   ğŸ¯ Custom generation with causal constraints")
            print("   ğŸ”§ Full control over architecture and training")
            print("   âš¡ Optimized for causal neuro-symbolic reasoning")
            
            return True
        else:
            print("âŒ Some components failed during testing")
            return False
            
    except Exception as e:
        print(f"âŒ CRITICAL ERROR: {e}")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
